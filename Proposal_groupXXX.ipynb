{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "- Alexandra Hernandez\n",
    "- Colin Kavanagh\n",
    "- Lily Qiu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Our origins are essential to our identity. Understanding someone’s origin is critical to understanding who they are and how to interact with them. However, computers currently struggle to account for this crucial aspect of human interaction. As artificial intelligence begins to take over more and more occupations, understanding human beings becomes even more of an essential task. In our project, we ask, “To what extent can unsupervised machine learning methods discriminate different types of accented English”? Our dataset uses thousands of audio files labeled with the speaker's accent. We want to see how well we can use clustering algorithms to group different accented English types and compare them with their proper labels. We also want to compare how well different algorithms cluster, using metrics like purity and the Rand Index to measure how well the clusters cluster concerning their true label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Unsupervised learning has allowed us to do many things, such as detect anomalies or allow computers to see and analyze images. A typical application of unsupervised learning is when it comes to natural language processing. For example, researchers used unsupervised learning techniques to teach machines to find patterns<a id=\"noteone\"></a>[<sup>[1]</sup>](#onenote). They used an algorithm called Automatic Distillation of Structure, which they used to teach a machine various things, from languages to protein sequencing. However, most of the inputs into the model they used were text data instead of audio data. Another group of researchers explored various unsupervised methods for teaching a machine natural language <a id=\"notetwo\"></a>[<sup>[2]</sup>](#twonote). This research examined and compared how machines learn to how humans learn languages. However, the author states that the unsupervised approach to machines learning language is not well studied, and there is still a lot of research to do about the approach. Finally, another research group attempted to use unsupervised learning to detect variations in English accents <a id=\"notethree\"></a>[<sup>[3]</sup>](#threenote).However, this research only accounted for accents localized to the United Kingdom. It did not account for accented English in other parts of the world. Also, many of the excerpts were, on average, 43 seconds long, and we would like to see if we can detect accents with less data. While these previous research ventures achieved excellent results, many did not consider the breadth of accented speech worldwide and their nuances. We want to explore accented English globally because it is more applicable to general human speech and nuances, which could be applied to speech recognition programs only trained in American English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Determining where someone is from can be important information in social contexts. Many people’s worldviews and norms are shaped by their origin and other aspects of their identity. While this may be subtle in our everyday lives, these differences are substantial in communication and understanding each other. As we transform our society into a more interconnected one through technologies like the internet, it becomes more and more imperative to understand unfamiliar cultures.<br>\n",
    "As a human being, it can be easy to determine whether someone’s accent is similar to our own through learned experience. Even if we can’t pinpoint exactly where someone is from, we can notice and understand that someone’s worldview and experiences may differ. Humans can account for these differences through changes in behavior to account for another person’s experiences. <br>\n",
    "However, as we rely on artificial intelligence to take over more and more occupations, there is a concern regarding professions that depend on a “human touch” to succeed. An example of this is replacing customer service jobs with robots, which has resulted in mass dissatisfaction among those who use the service. This is compared to humans who understand emotions, the scope of a problem, and “people first service.” Currently, we are not focusing on this “human-computer interaction”. <br>\n",
    "Our project proposes to solve a vital step in human-computer interaction, which involves understanding people as people instead of problems that need to be solved. In this project, we aim to determine where someone is from via samples of their accented English. We propose doing this through various unsupervised machine-learning clustering methods. Our solution can be applied to human-computer interaction problems to help computers better understand the people they are helping in the context of their cultural norms and beliefs. This project can be quantified by properly encoding audio files into vectors through the wave2vec library. The success of our methods can be quantified through metrics like purity and the Rand Index for matching predicted clusters with their true label.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "The dataset that's going to be used in this project is the **Common Voice** dataset created by [mozilla.org](https://commonvoice.mozilla.org/en/datasets), and contains 10 variables, with a total of 90,474 observations. The dataset contains speech data read by the site's users, and is made up of audio clips from the users. The entire dataset is divided into two groups: \"valid\" and \"other\". In order for an observation to fall under the \"valid\" group, the audio clip must have had at least 2 listeners, and the majority of those listeners classify it with the audio matching the speech text it's read from. If an observation doesn't fulfill this criteria, it falls under \"other\". Though the observations will be evaluated equally in this project, it's important to note that some may hold higher weight in the algorithms than others. </br>\n",
    "\n",
    "In the dataset, each observation includes `filename`,`text`,`up-votes`,`down-votes`,`age`,`gender`, and `accent`; however, in this project, the critical variables are `accent`, `up-votes`, and `down-votes`, since the aim is to cluster common voices based on their accent/locale. These critical variables are represented in the dataset by the user's reported region (e.g. \"United States English\"), the number of users who said that the speaker audio matched the speech text, and the number of users who said that the speaker audio did not match the speech text. </br>\n",
    "As for any special handling, we plan to convert each audio file into vectors using the `wave2vec` Python package [(github.com)](https://github.com/cristoper/wav2vec). Therefore, the audio files can be clustered properly once completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We will be comparing multiple unsupervised clustering algorithms on their accuracy. The distribution of the different accent audio data will determine which algorithm is best suited for separating the different accents. Separate the data into the training set and the test set. The following are the clustering algorithms we will train. The benchmark in which we will compare our generated clustering is the ground truth clustering of our data.\n",
    "\n",
    "#### K-means\n",
    "\n",
    "K-means clustering is a centroid based clustering. It assumes that data points of the same cluster is close based on euclidean distance. It works by minimizing the distance of each point to the center of its cluster. <br>\n",
    "Python implementation : <br>\n",
    "`from sklearn.cluster import KMeans` <br>\n",
    "`solution = KMeans(n_clusters = number_of_clusters).fit(data_points)`\n",
    "\n",
    "#### Gaussian Mixture model\n",
    "Gaussian Mixture models is a distribution based clustering alogorithm. It assumes data points of the same cluster is of the same distribution, with a certain mean and covariance. For each of the k clusters, it computes the optimal coefficient $\\pi_k$, mean $\\mu_k$, and covariance matrix $\\Sigma_k$ that maximizes the probability of X being in that particular cluster.\n",
    "\n",
    "#### Heirarchical Clustering\n",
    "Heirarchical clustering is meant for data that has a nested structure for similarity, in that a datapoint can be closer to another datapoint in some respects, but they can both be related to another group of datapoints in another respect. For example, accents from North American countries can be may be the most similar, but they may both be similar to the subset of accents from English speaking nations. This algorithm works by starting with each datapoint in its own cluster and iteratively merging pairs of clusters that have the minimimum distance.\n",
    "\n",
    "Ward linkage, measurement of cluster distance :\n",
    "$d(A, B) = \\frac{|A||B|}{|A| + |B|} ||\\mu_A - \\mu_B||^2 = \\underset{x \\in A \\cup B}{\\sum} ||x - \\mu_{A \\cup B}||^2 - \\underset{x \\in A}{\\sum} ||x - \\mu_A||^2 - \\underset{x \\in B}{\\sum} ||x - \\mu_B||^2$\n",
    "\n",
    "The caveat for a Hierarchical clustering model, it is not a classification algorithm, and thus we cannot make predictions about accents, given an audio file. It is simply a procedure to cluster a given set of datapoints.\n",
    "\n",
    "#### Spectral Clustering\n",
    "Spectral Clustering makes minimal assumptions about the shape of the data, but requires the number of clusters to be provided. It is suitable for this project because we know how many clusters there should be. It consists of representing a datset as a connected graph, with each node as a datapoint and each edge weight representing the similarity between datapoints. Represent the edge weights with matrix $W$, entry $w_{i, j}$ being the weight or similarity between datapoints i and j. The algorithm iteratively makes cuts (removes edges) until there are k connected components left, and the resulting graph represents the datapoints in each of the k clusters.\n",
    "\n",
    "Laplacian matrix $L_{n \\times n} = D - W$\n",
    "\n",
    "where $D$ is the degree matrix. The degree matrix is a diagonal matrix where each entry $d_{i, i}$ is the weighted degree of vertex $v_i$. In other words, the ith entry of the degree matrix is the sum of the weights of the edges that are adjacent to vertex $v_i$ <br>\n",
    "$W$ is the weighted adjacency matrix\n",
    "\n",
    "Like Hierarchical Clustering, Spectral Clustering is merely a procedure to cluster a given set of data, and cannot be used to make classifications.\n",
    "\n",
    "#### DBSCAN\n",
    "DBSCAN is a density based clustering that assumes clusters have a high density of data points. It does not require specifying the number of clusters, and can fit clusters of any shape. Before the procedure, define values eps, or the maxmimum distance in which 2 points can be considered neighbors, and minpts, the minimum number of neighbors within radius eps from a point. Core points are data points with more than minpts neighbors. Assign initial core points. 2 points are density connected if there are both within eps distance of a core point, and assign them to the same cluster as the core point. Iterate through every point in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Since we know the true labels of each datapoint, i.e. since Common Voice is a validated dataset, we know the true acccent of each audio file. Thus to obtain the most accurate model, we are able to use external evaulation metrics.\n",
    "\n",
    "One method is to run each model on the testing set and compute the proportion of misclassified audios through the measure of purity.\n",
    "\n",
    "$Purity = \\frac{1}{N} \\sum_{i = 1}^k max_j |c_i \\cap t_j|$\n",
    "\n",
    "Where $N$ is the number of datapoints, $k$ is the number of clusters, $c_i$ is the ith cluster, $t_j$ is a ground truth classification\n",
    "\n",
    "For each cluster $c_i$, we add the number of datapoints in the maximum overlap between it and each true category.\n",
    "\n",
    "Another metric of evalutaion of comparing the similarity of the generated clustering to the true clustering is the Adjusted Rand Index (ARI)\n",
    "\n",
    "$\\text{Rand Index (RI)} = \\frac{a + b}{n \\choose 2}$\n",
    "\n",
    "where $a$ is the number of pairs of elements belongs to the same cluster across the generated labels and the true labels <br>\n",
    "$b$ is the number of pairs of elements that belong to different clusters across the generated labels and the true labels <br>\n",
    "$n \\choose 2$ is the nubmer of possible pairs of elements\n",
    "\n",
    "`sklearn.metrics.adjusted_rand_score(labels_true, labels_pred)`\n",
    "\n",
    "An internal evaluation metric that does not rely on the true labels of datapoints is the Silhouette Coefficient\n",
    "\n",
    "#### Silhouette Coefficient\r\n",
    "\r\n",
    "One evaluation metric, which is standard for evaluating clustering algorithms, is the average silhouette coefficient, or silhouette score, which is calculated by the following <br>\r\n",
    "For each generated cluster, compute the following : <br>\r\n",
    "<br>\r\n",
    "$s = \\frac{b - a}{max(a, b)}$ <br>\r\n",
    "Where a is the average euclidean distance from a datapoint to all other points of the same cluster<br>\n",
    "And b is the average euclidean distance from a datapoint to all other points in the next nearest cluster <br>\n",
    "Then take the average across all clusters. The higher the silhouette score, the better the clustering algorithm. <br>\n",
    "\n",
    "We will generate this metric for all of the clustering algorithms we will use to determine which performs the best with audio data for accent detection. detection. detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset is open-source, and–therefore–permitted for public use, it's certain that the voices in the Common Voice dataset consented to be included; as a result, subjects included in the observations have informed consent. However, a identity concern may be the `filename` variable included in each observation, as they may include some identifying information of each user (e.g. their user IDs). To mitigate this problem, we won't include the `filename` variable in our evaluation, as it isn't considered a \"critical variable\". Excluding this part of the observations shouldn't hurt our outcome, and will protect the identity of each person included in the dataset. </br>\n",
    "\n",
    "In terms of biases, while the the age demographics of the observations being somewhat normally distributed, with a skew towards the young adult and middle age categories (20-29, 30-39, and 40-49 year olds), there is an *overwhelming* percentage of speakers that are male (45%), cutting female (17%) and other (2%) into a lower split. Despite having a considerable distribution of age, there's definitely collection bias present in this dataset, which may affect our evaluation outcome, as the pitch between the sexes could affect how the accents/locales are predicted. To counter this issue, we'll need to account for this confound when examining our results from the evaluations. </br>\n",
    "\n",
    "Lastly, to ensure data security and fair usage/viewing, the vectorization of the audio clips to optimize the observations, will further protect the data when completed and deployed on the respective project repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure the project's completeness and overall teamwork, our group expectations include:\n",
    "1. Communicating on our group channel (Discord) on planning, sectioning work, and deadlines.\n",
    "2. Holding each other accountable in completing the work before deadlines.\n",
    "3. Creating/maintaining a space where we all feel heard and understood in our ideas.\n",
    "4. Confirming when sections are completed so everyone can sync properly.\n",
    "5. Resolving any underlying disagreements before continuing on with a task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with something meaningful that is appropriate for your needs. It doesn't have to be something that fits this format.  It doesn't have to be set in stone... \"no battle plan survives contact with the enemy\". But you need a battle plan nonetheless, and you need to keep it updated so you understand what you are trying to accomplish, who's responsible for what, and what the expected due dates are for each item.\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/22  |  12 PM |  research how to vectorize audio data to use for clustering | complete vectorization | \n",
    "| 2/27  |  12 PM |  do EDA on audio data, visualizing the shape | discuss which models will be best suited for the data | \n",
    "| 2/29  | 12 PM  | do one form of clustering  | discuss the accuracy of the clustering   |\n",
    "| 3/5  | 12 PM  | do one form of clustering | discuss the accuracy of the clustering   |\n",
    "| 3/12  | 12 PM  | do one form of clustering | discuss the accuracy of the clustering |\n",
    "| 3/14  | 12 PM  | summarize the effectiveness of each clustering algorithm | finalize the report |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a id=\"onenote\"><a href=\"#noteone\">1. </a></a> Solan, Z., Horn, D., Ruppin, E., & Edelman, S. (2005). Unsupervised learning of natural languages. Proceedings of the National Academy of Sciences, 102(33), 11629-11634. https://doi.org/10.1073/pnas.0409746102<br>\n",
    "<a id=\"twonote\"><a href=\"#notetwo\">2. </a></a>: Klein, Dan. The unsupervised learning of natural language structure. Stanford:: Stanford University, 2005.<br>\n",
    "<a id=\"threenote\"><a href=\"#notethree\">3. </a></a>: Najafian, Maryam, et al. \"Unsupervised model selection for recognition of regional accented speech.\" Fifteenth annual conference of the international speech communication association. 2014.<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
