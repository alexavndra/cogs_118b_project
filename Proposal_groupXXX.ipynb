{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description (Delete this section when proposal is completed)\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like mtcars, iris, palmer penguins etc.\n",
    "- The dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training an unsupervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "\n",
    "- The project must include some elements of unsupervised learning, but you are welcome to include some supervised or other learning approaches as well.\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your ML system.\n",
    "- You will evaluate the performance of your ML system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "- Alexandra Hernandez\n",
    "- Colin Kavanagh\n",
    "- Lily Qiu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Our origins are essential to our identity. Understanding someone’s origin is critical to understanding who they are and how to interact with them. However, computers currently struggle to account for this crucial aspect of human interaction. As artificial intelligence begins to take over more and more occupations, understanding human beings becomes even more of an essential task. In our project, we ask, “To what extent can unsupervised machine learning methods discriminate different types of accented English”? Our dataset uses thousands of audio files labeled with the speaker's accent. We want to see how well we can use clustering algorithms to group different accented English types and compare them with their proper labels. We also want to compare how well different algorithms cluster, using metrics like purity and the Rand Index to measure how well the clusters cluster concerning their true label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Unsupervised learning has allowed us to do many things, such as detect anomalies or allow computers to see and analyze images. A typical application of unsupervised learning is when it comes to natural language processing. For example, researchers used unsupervised learning techniques to teach machines to find patterns<a id=\"noteone\"></a>[<sup>[1]</sup>](#onenote). They used an algorithm called Automatic Distillation of Structure, which they used to teach a machine various things, from languages to protein sequencing. However, most of the inputs into the model they used were text data instead of audio data. Another group of researchers explored various unsupervised methods for teaching a machine natural language <a id=\"notetwo\"></a>[<sup>[2]</sup>](#twonote). This research examined and compared how machines learn to how humans learn languages. However, the author states that the unsupervised approach to machines learning language is not well studied, and there is still a lot of research to do about the approach. Finally, another research group attempted to use unsupervised learning to detect variations in English accents <a id=\"notethree\"></a>[<sup>[3]</sup>](#threenote).However, this research only accounted for accents localized to the United Kingdom. It did not account for accented English in other parts of the world. Also, many of the excerpts were, on average, 43 seconds long, and we would like to see if we can detect accents with less data. While these previous research ventures achieved excellent results, many did not consider the breadth of accented speech worldwide and their nuances. We want to explore accented English globally because it is more applicable to general human speech and nuances, which could be applied to speech recognition programs only trained in American English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Determining where someone is from can be important information in social contexts. Many people’s worldviews and norms are shaped by their origin and other aspects of their identity. While this may be subtle in our everyday lives, these differences are substantial in communication and understanding each other. As we transform our society into a more interconnected one through technologies like the internet, it becomes more and more imperative to understand unfamiliar cultures. \n",
    "As a human being, it can be easy to determine whether someone’s accent is similar to our own through learned experience. Even if we can’t pinpoint exactly where someone is from, we can notice and understand that someone’s worldview and experiences may differ. Humans can account for these differences through changes in behavior to account for another person’s experiences. \n",
    "However, as we rely on artificial intelligence to take over more and more occupations, there is a concern regarding professions that depend on a “human touch” to succeed. An example of this is replacing customer service jobs with robots, which has resulted in mass dissatisfaction among those who use the service. This is compared to humans who understand emotions, the scope of a problem, and “people first service.” Currently, we are not focusing on this “human-computer interaction”. \n",
    "Our project proposes to solve a vital step in human-computer interaction, which involves understanding people as people instead of problems that need to be solved. In this project, we aim to determine where someone is from via samples of their accented English. We propose doing this through various unsupervised machine-learning clustering methods. Our solution can be applied to human-computer interaction problems to help computers better understand the people they are helping in the context of their cultural norms and beliefs. This project can be quantified by properly encoding audio files into vectors through the wave2vec library. The success of our methods can be quantified through metrics like purity and the Rand Index for matching predicted clusters with their true label.  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "The dataset that's going to be used in this project is the **Common Voice** dataset created by [mozilla.org](https://commonvoice.mozilla.org/en/datasets), and contains 10 variables, with a total of 90,474 observations. The dataset contains speech data read by the site's users, and is made up of audio clips from the users. The entire dataset is divided into two groups: \"valid\" and \"other\". In order for an observation to fall under the \"valid\" group, the audio clip must have had at least 2 listeners, and the majority of those listeners classify it with the audio matching the speech text it's read from. If an observation doesn't fulfill this criteria, it falls under \"other\". Though the observations will be evaluated equally in this project, it's important to note that some may hold higher weight in the algorithms than others. </br>\n",
    "\n",
    "In the dataset, each observation includes `filename`,`text`,`up-votes`,`down-votes`,`age`,`gender`, and `accent`; however, in this project, the critical variables are `accent`, `up-votes`, and `down-votes`, since the aim is to cluster common voices based on their accent/locale. These critical variables are represented in the dataset by the user's reported region (e.g. \"United States English\"), the number of users who said that the speaker audio matched the speech text, and the number of users who said that the speaker audio did not match the speech text. </br>\n",
    "As for any special handling, we plan to convert each audio file into vectors using the `wave2vec` Python package [(github.com)](https://github.com/cristoper/wav2vec). Therefore, the audio files can be clustered properly once completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "~~In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Why might your solution work? Make sure to describe how the solution will be tested.~~\n",
    "\n",
    "~~If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.~~\n",
    "\n",
    "~~If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared.~~\n",
    "\n",
    "\n",
    "\n",
    "There is no one standard benchmark model for accent detection from audio data. Thus, we will be comparing multiple unsupervised clustering algorithms on thier accuracy. The distribution of the different accent audio data will determine which algorithm is best suited for separating the different accents. Separate the data into the training set and the test set. The following are the clustering algorithms we will train.\n",
    "\n",
    "#### K-means\n",
    "\n",
    "K-means clustering is a centroid based clustering. It assumes that data points of the same cluster is close based on euclidean distance. It works by minimizing the distance of each point to the center of its cluster. <br>\n",
    "Python implementation : <br>\n",
    "`from sklearn.cluster import KMeans` <br>\n",
    "`solution = KMeans(n_clusters = number_of_clusters).fit(data_points)`\n",
    "\n",
    "#### Gaussian Mixture model\n",
    "Gaussian Mixture models is a distribution based clustering alogorithm. It assumes data points of the same cluster is of the same distribution, with a certain mean and covariance. For each of the k clusters, it computes the optimal coefficient $\\pi_k$, mean $\\mu_k$, and covariance matrix $\\Sigma_k$ that maximizes the probability of X being in that particular cluster.\n",
    "\n",
    "#### Heirarchical Clustering\n",
    "Heirarchical clustering is meant for data that has a nested structure for similarity, in that a datapoint can be closer to another datapoint in some respects, but they can both be related to another group of datapoints in another respect. For example, accents from North American countries can be may be the most similar, but they may both be similar to the subset of accents from English speaking nations. This algorithm works by starting with each datapoint in its own cluster and iteratively merging pairs of clusters that have the minimimum distance.\n",
    "\n",
    "Ward linkage, measurement of cluster distance :\n",
    "$d(A, B) = \\frac{|A||B|}{|A| + |B|} ||\\mu_A - \\mu_B||^2 = \\underset{x \\in A \\cup B}{\\sum} ||x - \\mu_{A \\cup B}||^2 - \\underset{x \\in A}{\\sum} ||x - \\mu_A||^2 - \\underset{x \\in B}{\\sum} ||x - \\mu_B||^2$\n",
    "\n",
    "The caveat for a Hierarchical clustering model, it is not a classification algorithm, and thus we cannot make predictions about accents, given an audio file. It is simply a procedure to cluster a given set of datapoints. \n",
    "\n",
    "#### DBSCAN\n",
    "Density based clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "~~Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms).~~\n",
    "\n",
    "Since we know the true labels of each datapoint, i.e. since Common Voice is a validated dataset, we know the true acccent of each audio file. Thus to obtain the most accurate model, one method is to run each model on the testing set and compute the proportion of misclassified audios through the measure of purity.\n",
    "\n",
    "$Purity = \\frac{1}{N} \\sum_{i = 1}^k max_j |c_i \\cap t_j|$\n",
    "\n",
    "Where $N$ is the number of datapoints, $k$ is the number of clusters, $c_i$ is the ith cluster, $t_j$ is a ground truth classification\n",
    "\n",
    "For each cluster $c_i$, we add the number of datapoints in the maximum overlap between it and each true category.\n",
    "\n",
    "Another metric of evalutaion of comparing the similarity of the generated clustering to the true clustering is the Adjusted Rand Index (ARI)\n",
    "\n",
    "$\\text{Rand Index (RI)} = \\frac{}{}$\n",
    "\n",
    "`sklearn.metrics.adjusted_rand_score(labels_true, labels_pred)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset is open-source, and–therefore–permitted for public use, it's certain that the voices in the Common Voice dataset consented to be included; as a result, subjects included in the observations have informed consent. However, a identity concern may be the `filename` variable included in each observation, as they may include some identifying information of each user (e.g. their user IDs). To mitigate this problem, we won't include the `filename` variable in our evaluation, as it isn't considered a \"critical variable\". Excluding this part of the observations shouldn't hurt our outcome, and will protect the identity of each person included in the dataset. </br>\n",
    "\n",
    "In terms of biases, while the the age demographics of the observations being somewhat normally distributed, with a skew towards the young adult and middle age categories (20-29, 30-39, and 40-49 year olds), there is an *overwhelming* percentage of speakers that are male (45%), cutting female (17%) and other (2%) into a lower split. Despite having a considerable distribution of age, there's definitely collection bias present in this dataset, which may affect our evaluation outcome, as the pitch between the sexes could affect how the accents/locales are predicted. To counter this issue, we'll need to account for this confound when examining our results from the evaluations. </br>\n",
    "\n",
    "Lastly, to ensure data security and fair usage/viewing, the vectorization of the audio clips to optimize the observations, will further protect the data when completed and deployed on the respective project repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure the project's completeness and overall teamwork, our group expectations include:\n",
    "1. Communicating on our group channel (Discord) on planning, sectioning work, and deadlines.\n",
    "2. Holding each other accountable in completing the work before deadlines.\n",
    "3. Creating/maintaining a space where we all feel heard and understood in our ideas.\n",
    "4. Confirming when sections are completed so everyone can sync properly.\n",
    "5. Resolving any underlying disagreements before continuing on with a task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with something meaningful that is appropriate for your needs. It doesn't have to be something that fits this format.  It doesn't have to be set in stone... \"no battle plan survives contact with the enemy\". But you need a battle plan nonetheless, and you need to keep it updated so you understand what you are trying to accomplish, who's responsible for what, and what the expected due dates are for each item.\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic (Pelé) | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets (Beckenbaur)  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/14  | 6 PM  | Import & Wrangle Data ,do some EDA (Maradonna) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/23  | 12 PM  | Finalize wrangling/EDA; Begin programming for project (Cruyff) | Discuss/edit project code; Complete project |\n",
    "| 3/13  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Carlos)| Discuss/edit full project |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a id=\"onenote\"><a href=\"#noteone\">1. </a></a> Solan, Z., Horn, D., Ruppin, E., & Edelman, S. (2005). Unsupervised learning of natural languages. Proceedings of the National Academy of Sciences, 102(33), 11629-11634. https://doi.org/10.1073/pnas.0409746102<br>\n",
    "<a id=\"twonote\"><a href=\"#notetwo\">2. </a></a>: Klein, Dan. The unsupervised learning of natural language structure. Stanford:: Stanford University, 2005.<br>\n",
    "<a id=\"threenote\"><a href=\"#notethree\">3. </a></a>: Najafian, Maryam, et al. \"Unsupervised model selection for recognition of regional accented speech.\" Fifteenth annual conference of the international speech communication association. 2014.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
